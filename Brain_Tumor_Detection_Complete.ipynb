{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Brain Tumor Detection System\n",
        "## Complete CRISP-DM Methodology Implementation\n",
        "\n",
        "This notebook provides a comprehensive, step-by-step implementation of the Brain Tumor Detection (BTD) system following the CRISP-DM methodology. It includes all phases from data understanding through evaluation, with detailed documentation, code, and visualizations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 1: Business Understanding\n",
        "\n",
        "### Project Overview\n",
        "\n",
        "The Brain Tumor Detection (BTD) system is designed to analyze Magnetic Resonance Imaging (MRI) scans to detect and classify brain tumors. The system addresses a critical medical challenge where early and accurate diagnosis significantly impacts patient survival rates.\n",
        "\n",
        "### Problem Statement\n",
        "\n",
        "**Challenges:**\n",
        "- **Variability**: Tumor size, shape, and position vary significantly between patients\n",
        "- **Complexity**: Difficult to detect tumors without detailed knowledge of their properties\n",
        "- **Urgency**: Brain tumors grow rapidly, doubling in size approximately every 25 days\n",
        "- **Risk**: Misdiagnosis leads to inappropriate medical intervention, reducing survival chances\n",
        "\n",
        "### System Objectives\n",
        "\n",
        "1. **Detection**: Identify the presence or absence of brain tumors in MRI scans\n",
        "2. **Classification**: Classify tumor types (Glioma, Meningioma, Pituitary, No Tumor)\n",
        "3. **Accuracy**: Achieve high classification accuracy to support clinical decisions\n",
        "4. **Speed**: Provide fast detection results for timely medical intervention\n",
        "\n",
        "### Success Criteria\n",
        "\n",
        "- High accuracy in tumor detection (>90% target)\n",
        "- Reliable classification of tumor types\n",
        "- Fast processing time for clinical workflow\n",
        "- Robust performance across diverse patient demographics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 2: Data Understanding\n",
        "\n",
        "### Overview\n",
        "\n",
        "In this phase, we analyze the MRI brain tumor dataset to understand its characteristics, quality, and distribution. This includes:\n",
        "- Dataset structure and organization\n",
        "- Image counts and class distribution\n",
        "- Image properties (dimensions, formats, file sizes)\n",
        "- Data quality assessment\n",
        "- Visualizations of sample images and statistics\n",
        "\n",
        "### Dataset Information\n",
        "\n",
        "- **Source**: Kaggle - Brain Tumor Classification (MRI)\n",
        "- **Format**: JPEG images\n",
        "- **Classes**: 4 classes (Glioma, Meningioma, Pituitary, No Tumor)\n",
        "- **Original Split**: Training (87.9%) / Testing (12.1%)\n",
        "- **Decision**: Merge datasets and use random 80/20 train/validation split\n",
        "\n",
        "### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Running locally\n",
            "\n",
            "================================================================================\n",
            "GPU CONFIGURATION\n",
            "================================================================================\n",
            "⚠ No GPU detected, using CPU\n",
            "\n",
            "✓ All libraries imported and configured successfully!\n",
            "✓ TensorFlow version: 2.20.0\n",
            "✓ Dataset path: ../dataset\n",
            "✓ Output directory: ./output\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# SETUP AND CONFIGURATION\n",
        "# ============================================================================\n",
        "# This cell sets up the environment, detects GPU, and imports all libraries\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Detect if running on Kaggle\n",
        "IS_KAGGLE = os.path.exists('/kaggle/input')\n",
        "if IS_KAGGLE:\n",
        "    # Kaggle paths\n",
        "    DATASET_PATH = '/kaggle/input/brain-tumor-classification-mri'\n",
        "    OUTPUT_DIR = '/kaggle/working'\n",
        "    print(\"✓ Running on Kaggle\")\n",
        "else:\n",
        "    # Local paths\n",
        "    DATASET_PATH = '../dataset'\n",
        "    OUTPUT_DIR = './output'\n",
        "    print(\"✓ Running locally\")\n",
        "\n",
        "# Create output directories\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(f'{OUTPUT_DIR}/assets', exist_ok=True)\n",
        "os.makedirs(f'{OUTPUT_DIR}/models', exist_ok=True)\n",
        "\n",
        "# Import all necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report,\n",
        "    roc_curve, auc, roc_auc_score,\n",
        "    precision_recall_curve, average_precision_score\n",
        ")\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# TensorFlow/Keras imports\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import (\n",
        "    EarlyStopping, ModelCheckpoint, ReduceLROnPlateau,\n",
        "    CSVLogger, TensorBoard\n",
        ")\n",
        "\n",
        "# Configure GPU for optimal performance\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"GPU CONFIGURATION\")\n",
        "print(\"=\"*80)\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    print(f\"✓ {len(gpus)} GPU(s) detected\")\n",
        "    for i, gpu in enumerate(gpus):\n",
        "        print(f\"  GPU {i}: {gpu.name}\")\n",
        "    # Enable memory growth to avoid OOM errors\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"✓ GPU memory growth enabled\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"⚠ GPU configuration error: {e}\")\n",
        "    \n",
        "    # Set mixed precision for faster training (if supported)\n",
        "    try:\n",
        "        policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
        "        tf.keras.mixed_precision.set_global_policy(policy)\n",
        "        print(\"✓ Mixed precision training enabled (float16)\")\n",
        "    except:\n",
        "        print(\"⚠ Mixed precision not available, using float32\")\n",
        "else:\n",
        "    print(\"⚠ No GPU detected, using CPU\")\n",
        "\n",
        "# Set visualization style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "print(\"\\n✓ All libraries imported and configured successfully!\")\n",
        "print(f\"✓ TensorFlow version: {tf.__version__}\")\n",
        "print(f\"✓ Dataset path: {DATASET_PATH}\")\n",
        "print(f\"✓ Output directory: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Data Understanding analyzer initialized!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# PHASE 2: DATA UNDERSTANDING - IMPLEMENTATION\n",
        "# ============================================================================\n",
        "# All code is embedded here - no external imports needed\n",
        "\n",
        "class DataUnderstanding:\n",
        "    \"\"\"Class to analyze and understand the brain tumor MRI dataset.\"\"\"\n",
        "    \n",
        "    def __init__(self, dataset_path):\n",
        "        self.dataset_path = Path(dataset_path)\n",
        "        self.training_path = self.dataset_path / 'Training'\n",
        "        self.testing_path = self.dataset_path / 'Testing'\n",
        "        \n",
        "        self.stats = {\n",
        "            'training': defaultdict(int),\n",
        "            'testing': defaultdict(int),\n",
        "            'image_properties': {\n",
        "                'dimensions': [],\n",
        "                'file_sizes': [],\n",
        "                'formats': defaultdict(int),\n",
        "                'channels': defaultdict(int)\n",
        "            },\n",
        "            'corrupted_images': [],\n",
        "            'class_distribution': defaultdict(int)\n",
        "        }\n",
        "    \n",
        "    def check_dataset_structure(self):\n",
        "        \"\"\"Check if dataset directory structure is correct.\"\"\"\n",
        "        print(\"=\" * 80)\n",
        "        print(\"DATASET STRUCTURE CHECK\")\n",
        "        print(\"=\" * 80)\n",
        "        \n",
        "        if not self.dataset_path.exists():\n",
        "            print(f\"❌ ERROR: Dataset path '{self.dataset_path}' does not exist!\")\n",
        "            return False\n",
        "        \n",
        "        print(f\"✓ Dataset path found: {self.dataset_path}\")\n",
        "        \n",
        "        if self.training_path.exists():\n",
        "            print(f\"✓ Training directory found\")\n",
        "        else:\n",
        "            print(f\"❌ Training directory not found\")\n",
        "            return False\n",
        "        \n",
        "        if self.testing_path.exists():\n",
        "            print(f\"✓ Testing directory found\")\n",
        "        else:\n",
        "            print(f\"❌ Testing directory not found\")\n",
        "            return False\n",
        "        \n",
        "        expected_classes = ['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor']\n",
        "        for split in ['Training', 'Testing']:\n",
        "            split_path = self.dataset_path / split\n",
        "            if split_path.exists():\n",
        "                print(f\"\\n{split} classes:\")\n",
        "                for class_name in expected_classes:\n",
        "                    class_path = split_path / class_name\n",
        "                    if class_path.exists():\n",
        "                        count = len(list(class_path.glob('*.jpg'))) + len(list(class_path.glob('*.jpeg'))) + len(list(class_path.glob('*.png')))\n",
        "                        print(f\"  ✓ {class_name}: {count} images\")\n",
        "        \n",
        "        return True\n",
        "    \n",
        "    def count_images(self):\n",
        "        \"\"\"Count images in each class.\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"IMAGE COUNT ANALYSIS\")\n",
        "        print(\"=\" * 80)\n",
        "        \n",
        "        splits = {'Training': self.training_path, 'Testing': self.testing_path}\n",
        "        total_images = 0\n",
        "        \n",
        "        for split_name, split_path in splits.items():\n",
        "            if not split_path.exists():\n",
        "                continue\n",
        "            \n",
        "            print(f\"\\n{split_name} Set:\")\n",
        "            print(\"-\" * 80)\n",
        "            \n",
        "            class_dirs = [d for d in split_path.iterdir() if d.is_dir()]\n",
        "            for class_dir in sorted(class_dirs):\n",
        "                class_name = class_dir.name\n",
        "                image_files = list(class_dir.glob('*.jpg')) + list(class_dir.glob('*.jpeg')) + list(class_dir.glob('*.png'))\n",
        "                count = len(image_files)\n",
        "                \n",
        "                self.stats[split_name.lower()][class_name] = count\n",
        "                self.stats['class_distribution'][class_name] += count\n",
        "                total_images += count\n",
        "                \n",
        "                print(f\"  {class_name:25s}: {count:4d} images\")\n",
        "            \n",
        "            split_total = sum(self.stats[split_name.lower()].values())\n",
        "            print(f\"  {'TOTAL':25s}: {split_total:4d} images\")\n",
        "        \n",
        "        print(f\"\\n{'=' * 80}\")\n",
        "        print(f\"GRAND TOTAL: {total_images} images\")\n",
        "        print(\"=\" * 80)\n",
        "        \n",
        "        return total_images\n",
        "    \n",
        "    def analyze_image_properties(self, sample_size=None):\n",
        "        \"\"\"Analyze properties of images in the dataset.\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"IMAGE PROPERTIES ANALYSIS\")\n",
        "        print(\"=\" * 80)\n",
        "        \n",
        "        splits = {'Training': self.training_path, 'Testing': self.testing_path}\n",
        "        all_images = []\n",
        "        corrupted_count = 0\n",
        "        \n",
        "        for split_path in splits.values():\n",
        "            if not split_path.exists():\n",
        "                continue\n",
        "            for class_dir in split_path.iterdir():\n",
        "                if class_dir.is_dir():\n",
        "                    image_files = list(class_dir.glob('*.jpg')) + list(class_dir.glob('*.jpeg')) + list(class_dir.glob('*.png'))\n",
        "                    all_images.extend(image_files)\n",
        "        \n",
        "        if sample_size and sample_size < len(all_images):\n",
        "            import random\n",
        "            random.seed(42)\n",
        "            all_images = random.sample(all_images, sample_size)\n",
        "            print(f\"Sampling {sample_size} images for analysis...\")\n",
        "        else:\n",
        "            print(f\"Analyzing all {len(all_images)} images...\")\n",
        "        \n",
        "        print(\"\\nAnalyzing image properties...\")\n",
        "        \n",
        "        for idx, img_path in enumerate(all_images):\n",
        "            if (idx + 1) % 100 == 0:\n",
        "                print(f\"  Processed {idx + 1}/{len(all_images)} images...\", end='\\r')\n",
        "            \n",
        "            try:\n",
        "                file_size = img_path.stat().st_size / (1024 * 1024)\n",
        "                self.stats['image_properties']['file_sizes'].append(file_size)\n",
        "                \n",
        "                ext = img_path.suffix.lower()\n",
        "                self.stats['image_properties']['formats'][ext] += 1\n",
        "                \n",
        "                with Image.open(img_path) as img:\n",
        "                    width, height = img.size\n",
        "                    self.stats['image_properties']['dimensions'].append((width, height))\n",
        "                    \n",
        "                    if img.mode == 'RGB':\n",
        "                        channels = 3\n",
        "                    elif img.mode == 'L':\n",
        "                        channels = 1\n",
        "                    elif img.mode == 'RGBA':\n",
        "                        channels = 4\n",
        "                    else:\n",
        "                        channels = len(img.getbands())\n",
        "                    \n",
        "                    self.stats['image_properties']['channels'][channels] += 1\n",
        "            \n",
        "            except Exception as e:\n",
        "                corrupted_count += 1\n",
        "                self.stats['corrupted_images'].append({'path': str(img_path), 'error': str(e)})\n",
        "        \n",
        "        print(f\"\\n✓ Analysis complete! Processed {len(all_images)} images\")\n",
        "        if corrupted_count > 0:\n",
        "            print(f\"⚠ Found {corrupted_count} corrupted/unreadable images\")\n",
        "        \n",
        "        return len(all_images)\n",
        "    \n",
        "    def visualize_sample_images(self, num_samples=4):\n",
        "        \"\"\"Visualize sample images from each class.\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"SAMPLE IMAGES VISUALIZATION\")\n",
        "        print(\"=\" * 80)\n",
        "        \n",
        "        splits = {'Training': self.training_path, 'Testing': self.testing_path}\n",
        "        fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "        fig.suptitle('Sample Images from Each Class', fontsize=16, fontweight='bold')\n",
        "        \n",
        "        class_names = ['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor']\n",
        "        \n",
        "        for col, class_name in enumerate(class_names):\n",
        "            train_path = self.training_path / class_name\n",
        "            if train_path.exists():\n",
        "                train_images = list(train_path.glob('*.jpg')) + list(train_path.glob('*.jpeg')) + list(train_path.glob('*.png'))\n",
        "                if train_images:\n",
        "                    img = Image.open(train_images[0])\n",
        "                    axes[0, col].imshow(img, cmap='gray' if img.mode == 'L' else None)\n",
        "                    axes[0, col].set_title(f'Training: {class_name}', fontsize=10)\n",
        "                    axes[0, col].axis('off')\n",
        "            \n",
        "            test_path = self.testing_path / class_name\n",
        "            if test_path.exists():\n",
        "                test_images = list(test_path.glob('*.jpg')) + list(test_path.glob('*.jpeg')) + list(test_path.glob('*.png'))\n",
        "                if test_images:\n",
        "                    img = Image.open(test_images[0])\n",
        "                    axes[1, col].imshow(img, cmap='gray' if img.mode == 'L' else None)\n",
        "                    axes[1, col].set_title(f'Testing: {class_name}', fontsize=10)\n",
        "                    axes[1, col].axis('off')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{OUTPUT_DIR}/assets/sample_images.png', dpi=150, bbox_inches='tight')\n",
        "        print(f\"✓ Sample images saved to {OUTPUT_DIR}/assets/sample_images.png\")\n",
        "        plt.close()\n",
        "    \n",
        "    def visualize_class_distribution(self):\n",
        "        \"\"\"Create visualizations for class distribution.\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"CLASS DISTRIBUTION VISUALIZATIONS\")\n",
        "        print(\"=\" * 80)\n",
        "        \n",
        "        classes = sorted(self.stats['class_distribution'].keys())\n",
        "        train_counts = [self.stats['training'].get(c, 0) for c in classes]\n",
        "        test_counts = [self.stats['testing'].get(c, 0) for c in classes]\n",
        "        \n",
        "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "        \n",
        "        x = np.arange(len(classes))\n",
        "        width = 0.35\n",
        "        \n",
        "        axes[0].bar(x - width/2, train_counts, width, label='Training', color='#3498db')\n",
        "        axes[0].bar(x + width/2, test_counts, width, label='Testing', color='#e74c3c')\n",
        "        axes[0].set_xlabel('Class', fontsize=12)\n",
        "        axes[0].set_ylabel('Number of Images', fontsize=12)\n",
        "        axes[0].set_title('Image Count by Class and Split', fontsize=14, fontweight='bold')\n",
        "        axes[0].set_xticks(x)\n",
        "        axes[0].set_xticklabels([c.replace('_', ' ').title() for c in classes], rotation=45, ha='right')\n",
        "        axes[0].legend()\n",
        "        axes[0].grid(axis='y', alpha=0.3)\n",
        "        \n",
        "        total_counts = [train_counts[i] + test_counts[i] for i in range(len(classes))]\n",
        "        axes[1].pie(total_counts, labels=[c.replace('_', ' ').title() for c in classes],\n",
        "                    autopct='%1.1f%%', startangle=90, textprops={'fontsize': 10})\n",
        "        axes[1].set_title('Overall Class Distribution', fontsize=14, fontweight='bold')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{OUTPUT_DIR}/assets/class_distribution.png', dpi=150, bbox_inches='tight')\n",
        "        print(f\"✓ Class distribution plots saved to {OUTPUT_DIR}/assets/class_distribution.png\")\n",
        "        plt.close()\n",
        "    \n",
        "    def print_statistics(self):\n",
        "        \"\"\"Print comprehensive statistics.\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"DETAILED STATISTICS\")\n",
        "        print(\"=\" * 80)\n",
        "        \n",
        "        print(\"\\n1. IMAGE COUNTS BY CLASS AND SPLIT\")\n",
        "        print(\"-\" * 80)\n",
        "        print(f\"{'Class':<25} {'Training':<12} {'Testing':<12} {'Total':<12}\")\n",
        "        print(\"-\" * 80)\n",
        "        \n",
        "        all_classes = set()\n",
        "        all_classes.update(self.stats['training'].keys())\n",
        "        all_classes.update(self.stats['testing'].keys())\n",
        "        \n",
        "        for class_name in sorted(all_classes):\n",
        "            train_count = self.stats['training'].get(class_name, 0)\n",
        "            test_count = self.stats['testing'].get(class_name, 0)\n",
        "            total = train_count + test_count\n",
        "            print(f\"{class_name:<25} {train_count:<12} {test_count:<12} {total:<12}\")\n",
        "        \n",
        "        train_total = sum(self.stats['training'].values())\n",
        "        test_total = sum(self.stats['testing'].values())\n",
        "        grand_total = train_total + test_total\n",
        "        print(\"-\" * 80)\n",
        "        print(f\"{'TOTAL':<25} {train_total:<12} {test_total:<12} {grand_total:<12}\")\n",
        "        \n",
        "        print(\"\\n2. CLASS DISTRIBUTION (PERCENTAGES)\")\n",
        "        print(\"-\" * 80)\n",
        "        total = sum(self.stats['class_distribution'].values())\n",
        "        for class_name in sorted(self.stats['class_distribution'].keys()):\n",
        "            count = self.stats['class_distribution'][class_name]\n",
        "            percentage = (count / total) * 100 if total > 0 else 0\n",
        "            print(f\"{class_name:<25}: {count:4d} ({percentage:5.2f}%)\")\n",
        "        \n",
        "        if self.stats['image_properties']['dimensions']:\n",
        "            print(\"\\n3. IMAGE DIMENSIONS\")\n",
        "            print(\"-\" * 80)\n",
        "            dimensions = np.array(self.stats['image_properties']['dimensions'])\n",
        "            widths = dimensions[:, 0]\n",
        "            heights = dimensions[:, 1]\n",
        "            \n",
        "            print(f\"Width  - Min: {int(widths.min())}, Max: {int(widths.max())}, \"\n",
        "                  f\"Mean: {widths.mean():.1f}, Std: {widths.std():.1f}\")\n",
        "            print(f\"Height - Min: {int(heights.min())}, Max: {int(heights.max())}, \"\n",
        "                  f\"Mean: {heights.mean():.1f}, Std: {heights.std():.1f}\")\n",
        "    \n",
        "    def run_full_analysis(self, sample_size=None):\n",
        "        \"\"\"Run the complete data understanding analysis.\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"BRAIN TUMOR DETECTION - DATA UNDERSTANDING ANALYSIS\")\n",
        "        print(\"Phase 2: CRISP-DM Methodology\")\n",
        "        print(\"=\" * 80)\n",
        "        \n",
        "        if not self.check_dataset_structure():\n",
        "            print(\"\\n❌ Dataset structure check failed. Please verify the dataset path.\")\n",
        "            return\n",
        "        \n",
        "        total_images = self.count_images()\n",
        "        \n",
        "        if total_images > 0:\n",
        "            self.analyze_image_properties(sample_size=sample_size)\n",
        "        \n",
        "        self.print_statistics()\n",
        "        \n",
        "        if total_images > 0:\n",
        "            try:\n",
        "                self.visualize_sample_images()\n",
        "                self.visualize_class_distribution()\n",
        "            except Exception as e:\n",
        "                print(f\"\\n⚠ Warning: Could not generate visualizations: {e}\")\n",
        "        \n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"DATA UNDERSTANDING ANALYSIS COMPLETE!\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "# Initialize and run analysis\n",
        "analyzer = DataUnderstanding(dataset_path=DATASET_PATH)\n",
        "print(\"✓ Data Understanding analyzer initialized!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "BRAIN TUMOR DETECTION - DATA UNDERSTANDING ANALYSIS\n",
            "Phase 2: CRISP-DM Methodology\n",
            "================================================================================\n",
            "================================================================================\n",
            "DATASET STRUCTURE CHECK\n",
            "================================================================================\n",
            "✓ Dataset path found: ..\\dataset\n",
            "✓ Training directory found\n",
            "✓ Testing directory found\n",
            "\n",
            "Training classes:\n",
            "  ✓ glioma_tumor: 826 images\n",
            "  ✓ meningioma_tumor: 822 images\n",
            "  ✓ no_tumor: 395 images\n",
            "  ✓ pituitary_tumor: 827 images\n",
            "\n",
            "Testing classes:\n",
            "  ✓ glioma_tumor: 100 images\n",
            "  ✓ meningioma_tumor: 115 images\n",
            "  ✓ no_tumor: 105 images\n",
            "  ✓ pituitary_tumor: 74 images\n",
            "\n",
            "================================================================================\n",
            "IMAGE COUNT ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "Training Set:\n",
            "--------------------------------------------------------------------------------\n",
            "  glioma_tumor             :  826 images\n",
            "  meningioma_tumor         :  822 images\n",
            "  no_tumor                 :  395 images\n",
            "  pituitary_tumor          :  827 images\n",
            "  TOTAL                    : 2870 images\n",
            "\n",
            "Testing Set:\n",
            "--------------------------------------------------------------------------------\n",
            "  glioma_tumor             :  100 images\n",
            "  meningioma_tumor         :  115 images\n",
            "  no_tumor                 :  105 images\n",
            "  pituitary_tumor          :   74 images\n",
            "  TOTAL                    :  394 images\n",
            "\n",
            "================================================================================\n",
            "GRAND TOTAL: 3264 images\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "IMAGE PROPERTIES ANALYSIS\n",
            "================================================================================\n",
            "Analyzing all 3264 images...\n",
            "\n",
            "Analyzing image properties...\n",
            "  Processed 3200/3264 images...\n",
            "✓ Analysis complete! Processed 3264 images\n",
            "\n",
            "================================================================================\n",
            "DETAILED STATISTICS\n",
            "================================================================================\n",
            "\n",
            "1. IMAGE COUNTS BY CLASS AND SPLIT\n",
            "--------------------------------------------------------------------------------\n",
            "Class                     Training     Testing      Total       \n",
            "--------------------------------------------------------------------------------\n",
            "glioma_tumor              826          100          926         \n",
            "meningioma_tumor          822          115          937         \n",
            "no_tumor                  395          105          500         \n",
            "pituitary_tumor           827          74           901         \n",
            "--------------------------------------------------------------------------------\n",
            "TOTAL                     2870         394          3264        \n",
            "\n",
            "2. CLASS DISTRIBUTION (PERCENTAGES)\n",
            "--------------------------------------------------------------------------------\n",
            "glioma_tumor             :  926 (28.37%)\n",
            "meningioma_tumor         :  937 (28.71%)\n",
            "no_tumor                 :  500 (15.32%)\n",
            "pituitary_tumor          :  901 (27.60%)\n",
            "\n",
            "3. IMAGE DIMENSIONS\n",
            "--------------------------------------------------------------------------------\n",
            "Width  - Min: 174, Max: 1375, Mean: 467.1, Std: 132.8\n",
            "Height - Min: 167, Max: 1446, Mean: 469.8, Std: 124.7\n",
            "\n",
            "================================================================================\n",
            "SAMPLE IMAGES VISUALIZATION\n",
            "================================================================================\n",
            "✓ Sample images saved to ./output/assets/sample_images.png\n",
            "\n",
            "================================================================================\n",
            "CLASS DISTRIBUTION VISUALIZATIONS\n",
            "================================================================================\n",
            "✓ Class distribution plots saved to ./output/assets/class_distribution.png\n",
            "\n",
            "================================================================================\n",
            "DATA UNDERSTANDING ANALYSIS COMPLETE!\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Run complete data understanding analysis\n",
        "# This will:\n",
        "# 1. Check dataset structure\n",
        "# 2. Count images per class\n",
        "# 3. Analyze image properties\n",
        "# 4. Generate statistics\n",
        "# 5. Create visualizations\n",
        "\n",
        "analyzer.run_full_analysis()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Understanding Results Summary\n",
        "\n",
        "Based on the analysis, here are the key findings:\n",
        "\n",
        "**Dataset Statistics:**\n",
        "- Total images: 3,264\n",
        "- Classes: 4 (Glioma, Meningioma, Pituitary, No Tumor)\n",
        "- Image format: JPEG (RGB)\n",
        "- Image dimensions: Variable (mean ~467×470, most common 512×512)\n",
        "- File sizes: Mean 0.027 MB, Total 88.77 MB\n",
        "\n",
        "**Class Distribution:**\n",
        "- Glioma: 926 images (28.37%)\n",
        "- Meningioma: 937 images (28.71%)\n",
        "- Pituitary: 901 images (27.60%)\n",
        "- No Tumor: 500 images (15.32%)\n",
        "\n",
        "**Key Decisions:**\n",
        "1. Merge Training and Testing folders to address class disparities\n",
        "2. Use random 80/20 train/validation split during training\n",
        "3. Apply class weights to handle class imbalance\n",
        "4. Standardize image dimensions during preprocessing\n",
        "\n",
        "**Visualizations Generated:**\n",
        "- Sample images from each class\n",
        "- Class distribution charts\n",
        "- Image dimensions analysis\n",
        "- File properties analysis\n",
        "\n",
        "All visualizations are saved to `docs/assets/` directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 3: Data Preparation\n",
        "\n",
        "### Overview\n",
        "\n",
        "In this phase, we prepare the data for model training by:\n",
        "- Merging Training and Testing datasets\n",
        "- Implementing EfficientNet-specific preprocessing\n",
        "- Applying safe data augmentation (geometric only, preserves original colors)\n",
        "- Creating data generators with proper splitting\n",
        "- Calculating class weights to handle imbalance\n",
        "\n",
        "### Model Selection Rationale\n",
        "\n",
        "**Selected Models:**\n",
        "- **EfficientNet B2 (260×260)**: PRIMARY - Recommended for ~2,500 images, achieves 98-99.5% accuracy on similar MRI datasets\n",
        "- **EfficientNet B3 (300×300)**: ALTERNATIVE - Good balance, slightly larger than recommended\n",
        "- **EfficientNet B4 (380×380)**: EXPERIMENTAL - For 10k+ images, may risk overfitting on 3,264 images\n",
        "\n",
        "**Selection Criteria:**\n",
        "- Dataset size: 3,264 images (between 2,500 and 10k range)\n",
        "- Image dimensions: Mean ~467×470, most common 512×512\n",
        "- All selected models fit within 512×512 without upscaling\n",
        "\n",
        "### Preprocessing Strategy\n",
        "\n",
        "**Key Principles:**\n",
        "1. **Resize with padding** (not cropping) to preserve all image data\n",
        "2. **Black padding** is natural for MRI (borders are black, skull is grey)\n",
        "3. **EfficientNet preprocessing** for proper normalization\n",
        "4. **RGB format** (no conversion needed)\n",
        "\n",
        "### Data Augmentation Strategy\n",
        "\n",
        "**Safe Augmentation (Geometric Only - Preserves Original Colors):**\n",
        "- Zoom out only: 0.95-1.0 (very minimal, prevents cropping)\n",
        "- Translation: ±2% with black padding (very conservative, prevents cropping)\n",
        "- Rotation: ±3° with black padding (very conservative, prevents cropping)\n",
        "- Horizontal flip: Enabled (safe for brain MRI)\n",
        "- **NO brightness/contrast adjustment** - original MRI colors preserved\n",
        "\n",
        "### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ DataPreparation class defined!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# PHASE 3: DATA PREPARATION - CLASS DEFINITION\n",
        "# ============================================================================\n",
        "# Complete DataPreparation class with all methods embedded\n",
        "\n",
        "class DataPreparation:\n",
        "    \"\"\"Class to prepare and preprocess MRI brain tumor dataset for EfficientNet models.\"\"\"\n",
        "    \n",
        "    EFFICIENTNET_SIZES = {\n",
        "        'b2': 260,  # PRIMARY: Recommended for ~2,500 images\n",
        "        'b3': 300,  # ALTERNATIVE: Good balance\n",
        "        'b4': 380   # EXPERIMENTAL: For 10k+ images\n",
        "    }\n",
        "    \n",
        "    def __init__(self, dataset_path, model_variant='b2', merged_data_path=None):\n",
        "        self.dataset_path = Path(dataset_path)\n",
        "        self.training_path = self.dataset_path / 'Training'\n",
        "        self.testing_path = self.dataset_path / 'Testing'\n",
        "        self.model_variant = model_variant.lower()\n",
        "        \n",
        "        if self.model_variant not in self.EFFICIENTNET_SIZES:\n",
        "            raise ValueError(f\"Invalid model variant: {model_variant}\")\n",
        "        \n",
        "        self.input_size = self.EFFICIENTNET_SIZES[self.model_variant]\n",
        "        \n",
        "        if merged_data_path is None:\n",
        "            self.merged_data_path = self.dataset_path / 'merged'\n",
        "        else:\n",
        "            self.merged_data_path = Path(merged_data_path)\n",
        "        \n",
        "        print(f\"Using EfficientNet-{self.model_variant.upper()} with input size: {self.input_size}×{self.input_size}\")\n",
        "    \n",
        "    def _resize_with_padding(self, img_array, target_size):\n",
        "        \"\"\"Resize image with padding to maintain aspect ratio.\"\"\"\n",
        "        target_w, target_h = target_size\n",
        "        img_h, img_w = img_array.shape[:2]\n",
        "        \n",
        "        scale = min(target_w / img_w, target_h / img_h)\n",
        "        new_w = int(img_w * scale)\n",
        "        new_h = int(img_h * scale)\n",
        "        img_resized = tf.image.resize(img_array, (new_h, new_w), method='bilinear')\n",
        "        \n",
        "        pad_h = (target_h - new_h) // 2\n",
        "        pad_w = (target_w - new_w) // 2\n",
        "        \n",
        "        img_padded = tf.image.pad_to_bounding_box(\n",
        "            img_resized, pad_h, pad_w, target_h, target_w\n",
        "        )\n",
        "        \n",
        "        return img_padded.numpy()\n",
        "    \n",
        "    def merge_datasets(self, force_merge=False):\n",
        "        \"\"\"Merge Training and Testing folders into unified dataset.\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"MERGING TRAINING AND TESTING DATASETS\")\n",
        "        print(\"=\" * 80)\n",
        "        \n",
        "        if self.merged_data_path.exists() and not force_merge:\n",
        "            print(f\"✓ Merged dataset already exists at: {self.merged_data_path}\")\n",
        "            return self.merged_data_path\n",
        "        \n",
        "        self.merged_data_path.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        expected_classes = ['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor']\n",
        "        class_counts = defaultdict(int)\n",
        "        \n",
        "        for split_name, split_path in [('Training', self.training_path), ('Testing', self.testing_path)]:\n",
        "            if not split_path.exists():\n",
        "                continue\n",
        "            \n",
        "            print(f\"\\nProcessing {split_name}...\")\n",
        "            for class_name in expected_classes:\n",
        "                class_path = split_path / class_name\n",
        "                if not class_path.exists():\n",
        "                    continue\n",
        "                \n",
        "                merged_class_path = self.merged_data_path / class_name\n",
        "                merged_class_path.mkdir(exist_ok=True)\n",
        "                \n",
        "                image_files = list(class_path.glob('*.jpg')) + list(class_path.glob('*.jpeg')) + list(class_path.glob('*.png'))\n",
        "                \n",
        "                for img_file in image_files:\n",
        "                    new_name = f\"{split_name.lower()}_{img_file.name}\"\n",
        "                    dest_path = merged_class_path / new_name\n",
        "                    shutil.copy2(img_file, dest_path)\n",
        "                    class_counts[class_name] += 1\n",
        "                \n",
        "                print(f\"  {class_name}: {len(image_files)} images copied\")\n",
        "        \n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"MERGE SUMMARY\")\n",
        "        print(\"=\" * 80)\n",
        "        total = sum(class_counts.values())\n",
        "        for class_name in expected_classes:\n",
        "            print(f\"  {class_name}: {class_counts[class_name]} images\")\n",
        "        print(f\"  TOTAL: {total} images\")\n",
        "        print(f\"\\n✓ Merged dataset created at: {self.merged_data_path}\")\n",
        "        \n",
        "        return self.merged_data_path\n",
        "    \n",
        "    def create_train_datagen(self, validation_split=0.2):\n",
        "        \"\"\"Create training data generator with safe augmentation.\"\"\"\n",
        "        train_datagen = ImageDataGenerator(\n",
        "            zoom_range=[0.95, 1.0],  # Zoom out only\n",
        "            width_shift_range=0.02,   # ±2% horizontal shift\n",
        "            height_shift_range=0.02,  # ±2% vertical shift\n",
        "            fill_mode='constant',     # Pad with black\n",
        "            cval=0.0,                 # Black padding\n",
        "            rotation_range=3,         # ±3 degrees\n",
        "            horizontal_flip=True,\n",
        "            preprocessing_function=keras.applications.efficientnet.preprocess_input,\n",
        "            validation_split=validation_split\n",
        "        )\n",
        "        return train_datagen\n",
        "    \n",
        "    def create_test_datagen(self):\n",
        "        \"\"\"Create test data generator (no augmentation).\"\"\"\n",
        "        test_datagen = ImageDataGenerator(\n",
        "            preprocessing_function=keras.applications.efficientnet.preprocess_input\n",
        "        )\n",
        "        return test_datagen\n",
        "    \n",
        "    def create_data_generators(self, batch_size=32, seed=42, validation_split=0.2, use_merged=True):\n",
        "        \"\"\"Create train, validation, and test data generators.\"\"\"\n",
        "        if use_merged:\n",
        "            merged_path = self.merged_data_path\n",
        "            if not merged_path.exists():\n",
        "                self.merge_datasets()\n",
        "            data_path = merged_path\n",
        "        else:\n",
        "            data_path = self.training_path\n",
        "        \n",
        "        train_datagen = self.create_train_datagen(validation_split=validation_split)\n",
        "        \n",
        "        train_gen = train_datagen.flow_from_directory(\n",
        "            data_path,\n",
        "            target_size=(self.input_size, self.input_size),\n",
        "            batch_size=batch_size,\n",
        "            class_mode='categorical',\n",
        "            subset='training',\n",
        "            seed=seed,\n",
        "            shuffle=True\n",
        "        )\n",
        "        \n",
        "        val_datagen_with_split = ImageDataGenerator(\n",
        "            preprocessing_function=keras.applications.efficientnet.preprocess_input,\n",
        "            validation_split=validation_split\n",
        "        )\n",
        "        \n",
        "        val_gen = val_datagen_with_split.flow_from_directory(\n",
        "            data_path,\n",
        "            target_size=(self.input_size, self.input_size),\n",
        "            batch_size=batch_size,\n",
        "            class_mode='categorical',\n",
        "            subset='validation',\n",
        "            seed=seed,\n",
        "            shuffle=False\n",
        "        )\n",
        "        \n",
        "        test_datagen = self.create_test_datagen()\n",
        "        \n",
        "        if self.testing_path.exists() and any(self.testing_path.iterdir()):\n",
        "            test_gen = test_datagen.flow_from_directory(\n",
        "                self.testing_path,\n",
        "                target_size=(self.input_size, self.input_size),\n",
        "                batch_size=batch_size,\n",
        "                class_mode='categorical',\n",
        "                seed=seed,\n",
        "                shuffle=False\n",
        "            )\n",
        "        else:\n",
        "            test_gen = val_gen\n",
        "        \n",
        "        return train_gen, val_gen, test_gen\n",
        "    \n",
        "    def get_class_weights(self, train_gen, method='balanced'):\n",
        "        \"\"\"Calculate class weights to handle class imbalance.\"\"\"\n",
        "        class_counts = train_gen.classes\n",
        "        total_samples = len(class_counts)\n",
        "        num_classes = len(train_gen.class_indices)\n",
        "        \n",
        "        class_counts_dict = {}\n",
        "        for class_idx in range(num_classes):\n",
        "            class_counts_dict[class_idx] = np.sum(class_counts == class_idx)\n",
        "        \n",
        "        class_weights = {}\n",
        "        \n",
        "        if method == 'balanced':\n",
        "            for class_idx, count in class_counts_dict.items():\n",
        "                if count > 0:\n",
        "                    class_weights[class_idx] = total_samples / (num_classes * count)\n",
        "                else:\n",
        "                    class_weights[class_idx] = 0.0\n",
        "        \n",
        "        print(f\"\\nClass Weights (method: {method}):\")\n",
        "        print(\"-\" * 80)\n",
        "        for class_name, class_idx in sorted(train_gen.class_indices.items(), key=lambda x: x[1]):\n",
        "            count = class_counts_dict[class_idx]\n",
        "            weight = class_weights[class_idx]\n",
        "            pct = (count / total_samples) * 100 if total_samples > 0 else 0\n",
        "            print(f\"{class_name:<25} {count:<10} {weight:<10.3f} {pct:<10.1f}%\")\n",
        "        \n",
        "        return class_weights\n",
        "    \n",
        "    def visualize_augmentation(self, num_samples=8):\n",
        "        \"\"\"Visualize augmented images.\"\"\"\n",
        "        if (self.dataset_path / 'merged').exists():\n",
        "            sample_class_path = self.dataset_path / 'merged'\n",
        "        else:\n",
        "            sample_class_path = self.training_path\n",
        "        \n",
        "        sample_class = list(sample_class_path.iterdir())[0]\n",
        "        sample_image = list(sample_class.iterdir())[0]\n",
        "        \n",
        "        img = keras.utils.load_img(sample_image)\n",
        "        img_array = keras.utils.img_to_array(img)\n",
        "        img_array = self._resize_with_padding(img_array, (self.input_size, self.input_size))\n",
        "        img_array = np.expand_dims(img_array, axis=0)\n",
        "        \n",
        "        datagen = ImageDataGenerator(\n",
        "            zoom_range=[0.95, 1.0],\n",
        "            width_shift_range=0.02,\n",
        "            height_shift_range=0.02,\n",
        "            fill_mode='constant',\n",
        "            cval=0.0,\n",
        "            rotation_range=3,\n",
        "            horizontal_flip=True\n",
        "        )\n",
        "        \n",
        "        fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "        fig.suptitle(f'Data Augmentation Examples (EfficientNet-{self.model_variant.upper()})', fontsize=14)\n",
        "        axes = axes.flatten()\n",
        "        \n",
        "        original_display = img_array[0] / 255.0\n",
        "        axes[0].imshow(original_display)\n",
        "        axes[0].set_title('Original Image', fontsize=10)\n",
        "        axes[0].axis('off')\n",
        "        \n",
        "        aug_iter = datagen.flow(img_array, batch_size=1)\n",
        "        for i in range(1, num_samples):\n",
        "            aug_img = next(aug_iter)[0]\n",
        "            aug_img_display = aug_img / 255.0\n",
        "            aug_img_display = np.clip(aug_img_display, 0, 1)\n",
        "            axes[i].imshow(aug_img_display)\n",
        "            axes[i].set_title(f'Augmented {i}', fontsize=10)\n",
        "            axes[i].axis('off')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{OUTPUT_DIR}/assets/augmentation_examples_{self.model_variant}.png', dpi=150, bbox_inches='tight')\n",
        "        print(f\"\\n✓ Augmentation examples saved\")\n",
        "        plt.close()\n",
        "    \n",
        "    def print_preprocessing_summary(self):\n",
        "        \"\"\"Print summary of preprocessing configuration.\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"DATA PREPARATION SUMMARY\")\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"Model Variant: EfficientNet-{self.model_variant.upper()}\")\n",
        "        print(f\"Input Size: {self.input_size}×{self.input_size}\")\n",
        "        print(f\"Dataset Path: {self.dataset_path}\")\n",
        "        print(\"\\nPreprocessing:\")\n",
        "        print(\"  ✓ Resize with black padding (no cropping)\")\n",
        "        print(\"  ✓ EfficientNet preprocessing function\")\n",
        "        print(\"\\nAugmentation Strategy:\")\n",
        "        print(\"  ✓ Zoom out only: 0.95-1.0\")\n",
        "        print(\"  ✓ Translation: ±2% with black padding\")\n",
        "        print(\"  ✓ Rotation: ±3° with black padding\")\n",
        "        print(\"  ✓ Horizontal flip: Enabled\")\n",
        "        print(\"  ✓ NO brightness/contrast adjustment\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "print(\"✓ DataPreparation class defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Using embedded DataPreparation class\n"
          ]
        }
      ],
      "source": [
        "# Initialize data preparation (class is defined in previous cell)\n",
        "print(\"✓ Using embedded DataPreparation class\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using EfficientNet-B2 with input size: 260×260\n",
            "\n",
            "================================================================================\n",
            "DATA PREPARATION SUMMARY\n",
            "================================================================================\n",
            "Model Variant: EfficientNet-B2\n",
            "Input Size: 260×260\n",
            "Dataset Path: ..\\dataset\n",
            "\n",
            "Preprocessing:\n",
            "  ✓ Resize with black padding (no cropping)\n",
            "  ✓ EfficientNet preprocessing function\n",
            "\n",
            "Augmentation Strategy:\n",
            "  ✓ Zoom out only: 0.95-1.0\n",
            "  ✓ Translation: ±2% with black padding\n",
            "  ✓ Rotation: ±3° with black padding\n",
            "  ✓ Horizontal flip: Enabled\n",
            "  ✓ NO brightness/contrast adjustment\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Initialize data preparation for EfficientNet B2 (PRIMARY model)\n",
        "# You can change model_variant to 'b3' or 'b4' to test alternatives\n",
        "prep = DataPreparation(\n",
        "    dataset_path=DATASET_PATH,\n",
        "    model_variant='b2'  # PRIMARY: B2, ALTERNATIVE: B3, EXPERIMENTAL: B4\n",
        ")\n",
        "\n",
        "# Print preprocessing summary\n",
        "prep.print_preprocessing_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "MERGING TRAINING AND TESTING DATASETS\n",
            "================================================================================\n",
            "✓ Merged dataset already exists at: ..\\dataset\\merged\n",
            "\n",
            "✓ Merged dataset ready at: ..\\dataset\\merged\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Merge Training and Testing datasets\n",
        "merged_path = prep.merge_datasets(force_merge=False)\n",
        "print(f\"\\n✓ Merged dataset ready at: {merged_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "STEP 2: CREATING DATA GENERATORS\n",
            "================================================================================\n",
            "Found 2612 images belonging to 4 classes.\n",
            "Found 652 images belonging to 4 classes.\n",
            "Found 394 images belonging to 4 classes.\n",
            "\n",
            "✓ Training samples: 2612\n",
            "✓ Validation samples: 652\n",
            "✓ Test samples: 394\n",
            "✓ Batch size: 32\n",
            "\n",
            "Classes: {'glioma_tumor': 0, 'meningioma_tumor': 1, 'no_tumor': 2, 'pituitary_tumor': 3}\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Create data generators with train/validation split\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 2: CREATING DATA GENERATORS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Use larger batch size if GPU is available for faster training\n",
        "batch_size = 64 if gpus else 32\n",
        "\n",
        "train_gen, val_gen, test_gen = prep.create_data_generators(\n",
        "    batch_size=batch_size,\n",
        "    seed=42,  # Fixed seed for reproducibility\n",
        "    validation_split=0.2,  # 20% for validation\n",
        "    use_merged=True\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ Training samples: {train_gen.samples}\")\n",
        "print(f\"✓ Validation samples: {val_gen.samples}\")\n",
        "print(f\"✓ Test samples: {test_gen.samples}\")\n",
        "print(f\"✓ Batch size: {batch_size}\")\n",
        "print(f\"\\nClasses: {train_gen.class_indices}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Class Weights (method: balanced):\n",
            "--------------------------------------------------------------------------------\n",
            "glioma_tumor              741        0.881      28.4      %\n",
            "meningioma_tumor          750        0.871      28.7      %\n",
            "no_tumor                  400        1.633      15.3      %\n",
            "pituitary_tumor           721        0.906      27.6      %\n",
            "\n",
            "✓ Class weights calculated\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Calculate class weights to handle class imbalance\n",
        "class_weights = prep.get_class_weights(train_gen, method='balanced')\n",
        "print(f\"\\n✓ Class weights calculated\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✓ Augmentation examples saved\n",
            "\n",
            "✓ Augmentation examples saved\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Visualize data augmentation\n",
        "prep.visualize_augmentation(num_samples=8)\n",
        "print(\"\\n✓ Augmentation examples saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Preparation Summary\n",
        "\n",
        "**Completed Steps:**\n",
        "1. ✓ Merged Training and Testing datasets into unified dataset\n",
        "2. ✓ Created data generators with random 80/20 train/validation split\n",
        "3. ✓ Calculated class weights to handle class imbalance\n",
        "4. ✓ Visualized data augmentation (geometric only, preserves original colors)\n",
        "\n",
        "**Key Features:**\n",
        "- EfficientNet-specific preprocessing (resize to 260×260 for B2)\n",
        "- Safe augmentation that preserves all image content\n",
        "- Original MRI colors preserved (no brightness/contrast adjustments)\n",
        "- Black padding matches natural MRI appearance\n",
        "- Class weights applied to handle imbalance\n",
        "\n",
        "**Next Steps:**\n",
        "- Proceed to Phase 4: Modeling to train the EfficientNet model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ EfficientNetTrainer class defined!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# PHASE 4: MODELING - TRAINING CLASS DEFINITION\n",
        "# ============================================================================\n",
        "# Complete EfficientNetTrainer class with all methods embedded\n",
        "\n",
        "class EfficientNetTrainer:\n",
        "    \"\"\"Trainer class for EfficientNet models on brain tumor detection.\"\"\"\n",
        "    \n",
        "    EFFICIENTNET_VARIANTS = {\n",
        "        'b2': {'size': 260, 'name': 'EfficientNetB2'},\n",
        "        'b3': {'size': 300, 'name': 'EfficientNetB3'},\n",
        "        'b4': {'size': 380, 'name': 'EfficientNetB4'}\n",
        "    }\n",
        "    \n",
        "    def __init__(self, model_variant='b2', version='v1.0', dataset_path=None,\n",
        "                 base_model_trainable=False, dropout_rate=0.2):\n",
        "        if model_variant.lower() not in self.EFFICIENTNET_VARIANTS:\n",
        "            raise ValueError(f\"Model variant must be one of {list(self.EFFICIENTNET_VARIANTS.keys())}\")\n",
        "        \n",
        "        self.model_variant = model_variant.lower()\n",
        "        self.version = version\n",
        "        self.base_model_trainable = base_model_trainable\n",
        "        self.dropout_rate = dropout_rate\n",
        "        \n",
        "        self.input_size = self.EFFICIENTNET_VARIANTS[self.model_variant]['size']\n",
        "        self.model_name = self.EFFICIENTNET_VARIANTS[self.model_variant]['name']\n",
        "        \n",
        "        self.models_dir = Path(f'{OUTPUT_DIR}/models/efficientnet/{version}/efficientnet_{self.model_variant}')\n",
        "        self.models_dir.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        self.model = None\n",
        "        self.history = None\n",
        "    \n",
        "    def build_model(self):\n",
        "        \"\"\"Build EfficientNet model with transfer learning.\"\"\"\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"BUILDING MODEL\")\n",
        "        print(f\"{'='*80}\")\n",
        "        \n",
        "        # Load EfficientNet base model\n",
        "        if self.model_variant == 'b2':\n",
        "            base_model = keras.applications.EfficientNetB2(\n",
        "                weights='imagenet', include_top=False,\n",
        "                input_shape=(self.input_size, self.input_size, 3)\n",
        "            )\n",
        "        elif self.model_variant == 'b3':\n",
        "            base_model = keras.applications.EfficientNetB3(\n",
        "                weights='imagenet', include_top=False,\n",
        "                input_shape=(self.input_size, self.input_size, 3)\n",
        "            )\n",
        "        elif self.model_variant == 'b4':\n",
        "            base_model = keras.applications.EfficientNetB4(\n",
        "                weights='imagenet', include_top=False,\n",
        "                input_shape=(self.input_size, self.input_size, 3)\n",
        "            )\n",
        "        \n",
        "        base_model.trainable = self.base_model_trainable\n",
        "        \n",
        "        # Build model\n",
        "        inputs = keras.Input(shape=(self.input_size, self.input_size, 3))\n",
        "        x = keras.applications.efficientnet.preprocess_input(inputs)\n",
        "        x = base_model(x, training=False)\n",
        "        x = layers.GlobalAveragePooling2D()(x)\n",
        "        x = layers.Dropout(self.dropout_rate)(x)\n",
        "        \n",
        "        # Get num_classes from train_gen (will be set later)\n",
        "        num_classes = 4  # Default, will be updated\n",
        "        outputs = layers.Dense(num_classes, activation='softmax', name='predictions')(x)\n",
        "        \n",
        "        self.model = models.Model(inputs, outputs, name=f'EfficientNet-{self.model_variant.upper()}')\n",
        "        \n",
        "        print(f\"\\n✓ Model built successfully\")\n",
        "        print(f\"  Total parameters: {self.model.count_params():,}\")\n",
        "        \n",
        "        return self.model\n",
        "    \n",
        "    def compile_model(self, train_gen, learning_rate=1e-4, optimizer='adam'):\n",
        "        \"\"\"Compile the model.\"\"\"\n",
        "        if self.model is None:\n",
        "            self.build_model()\n",
        "        \n",
        "        # Update output layer for correct number of classes\n",
        "        num_classes = len(train_gen.class_indices)\n",
        "        if self.model.layers[-1].output_shape[-1] != num_classes:\n",
        "            # Rebuild output layer\n",
        "            x = self.model.layers[-2].output\n",
        "            outputs = layers.Dense(num_classes, activation='softmax', name='predictions')(x)\n",
        "            self.model = models.Model(self.model.input, outputs)\n",
        "        \n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"COMPILING MODEL\")\n",
        "        print(f\"{'='*80}\")\n",
        "        \n",
        "        if optimizer.lower() == 'adam':\n",
        "            opt = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "        elif optimizer.lower() == 'sgd':\n",
        "            opt = keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
        "        \n",
        "        # Custom F1-Score metric\n",
        "        class F1Score(keras.metrics.Metric):\n",
        "            def __init__(self, name='f1_score', **kwargs):\n",
        "                super().__init__(name=name, **kwargs)\n",
        "                self.precision = keras.metrics.Precision()\n",
        "                self.recall = keras.metrics.Recall()\n",
        "            \n",
        "            def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "                self.precision.update_state(y_true, y_pred, sample_weight)\n",
        "                self.recall.update_state(y_true, y_pred, sample_weight)\n",
        "            \n",
        "            def result(self):\n",
        "                p = self.precision.result()\n",
        "                r = self.recall.result()\n",
        "                return 2 * ((p * r) / (p + r + keras.backend.epsilon()))\n",
        "            \n",
        "            def reset_state(self):\n",
        "                self.precision.reset_state()\n",
        "                self.recall.reset_state()\n",
        "        \n",
        "        self.model.compile(\n",
        "            optimizer=opt,\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy', keras.metrics.Precision(name='precision'),\n",
        "                    keras.metrics.Recall(name='recall'), F1Score()]\n",
        "        )\n",
        "        \n",
        "        print(f\"✓ Optimizer: {optimizer}\")\n",
        "        print(f\"✓ Learning rate: {learning_rate}\")\n",
        "        print(f\"✓ Metrics: accuracy, precision, recall, f1_score\")\n",
        "    \n",
        "    def train(self, train_gen, val_gen, class_weights, epochs=50, batch_size=32,\n",
        "              learning_rate=1e-4, optimizer='adam', patience=10, min_lr=1e-7):\n",
        "        \"\"\"Train the model.\"\"\"\n",
        "        if self.model is None:\n",
        "            self.build_model()\n",
        "            self.compile_model(train_gen, learning_rate, optimizer)\n",
        "        \n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"TRAINING MODEL\")\n",
        "        print(f\"{'='*80}\")\n",
        "        print(f\"Epochs: {epochs}\")\n",
        "        print(f\"Batch size: {batch_size}\")\n",
        "        print(f\"Learning rate: {learning_rate}\")\n",
        "        \n",
        "        # Setup callbacks\n",
        "        callbacks = [\n",
        "            EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True, verbose=1),\n",
        "            ModelCheckpoint(filepath=str(self.models_dir / 'best_model.keras'),\n",
        "                          monitor='val_loss', save_best_only=True, verbose=1),\n",
        "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=patience//2,\n",
        "                            min_lr=min_lr, verbose=1),\n",
        "            CSVLogger(filename=str(self.models_dir / 'training_log.csv'))\n",
        "        ]\n",
        "        \n",
        "        # Train\n",
        "        print(f\"\\nStarting training...\")\n",
        "        self.history = self.model.fit(\n",
        "            train_gen,\n",
        "            epochs=epochs,\n",
        "            validation_data=val_gen,\n",
        "            class_weight=class_weights,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "        \n",
        "        print(f\"\\n✓ Training completed!\")\n",
        "        return self.history\n",
        "    \n",
        "    def evaluate(self, test_gen):\n",
        "        \"\"\"Evaluate the model on test set.\"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not trained.\")\n",
        "        \n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"EVALUATING MODEL\")\n",
        "        print(f\"{'='*80}\")\n",
        "        \n",
        "        test_results = self.model.evaluate(test_gen, verbose=1)\n",
        "        print(f\"\\nTest Results:\")\n",
        "        print(f\"  Loss: {test_results[0]:.4f}\")\n",
        "        print(f\"  Accuracy: {test_results[1]:.4f}\")\n",
        "        \n",
        "        return test_results\n",
        "    \n",
        "    def save_model(self):\n",
        "        \"\"\"Save the trained model.\"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not trained.\")\n",
        "        \n",
        "        save_path = self.models_dir / 'final_model.keras'\n",
        "        self.model.save(str(save_path))\n",
        "        print(f\"✓ Model saved to: {save_path}\")\n",
        "        \n",
        "        # Save config\n",
        "        config = {\n",
        "            'model_variant': self.model_variant,\n",
        "            'version': self.version,\n",
        "            'input_size': self.input_size,\n",
        "            'base_model_trainable': self.base_model_trainable,\n",
        "            'dropout_rate': self.dropout_rate\n",
        "        }\n",
        "        \n",
        "        config_path = self.models_dir / 'model_config.json'\n",
        "        with open(config_path, 'w') as f:\n",
        "            json.dump(config, f, indent=2)\n",
        "        \n",
        "        # Save history\n",
        "        if self.history is not None:\n",
        "            history_dict = {key: [float(val) for val in values] \n",
        "                           for key, values in self.history.history.items()}\n",
        "            history_path = self.models_dir / 'training_history.json'\n",
        "            with open(history_path, 'w') as f:\n",
        "                json.dump(history_dict, f, indent=2)\n",
        "            \n",
        "            # Visualize training curves\n",
        "            self.visualize_training_curves()\n",
        "        \n",
        "        return save_path\n",
        "    \n",
        "    def visualize_training_curves(self):\n",
        "        \"\"\"Visualize and save training curves.\"\"\"\n",
        "        if self.history is None:\n",
        "            return\n",
        "        \n",
        "        history = self.history.history\n",
        "        epochs = range(1, len(history['loss']) + 1)\n",
        "        \n",
        "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "        fig.suptitle(f'Training Curves - EfficientNet-{self.model_variant.upper()} (v{self.version})',\n",
        "                     fontsize=16, fontweight='bold')\n",
        "        \n",
        "        # Loss\n",
        "        axes[0, 0].plot(epochs, history['loss'], 'b-', label='Training Loss', linewidth=2)\n",
        "        if 'val_loss' in history:\n",
        "            axes[0, 0].plot(epochs, history['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
        "        axes[0, 0].set_xlabel('Epoch')\n",
        "        axes[0, 0].set_ylabel('Loss')\n",
        "        axes[0, 0].set_title('Model Loss')\n",
        "        axes[0, 0].legend()\n",
        "        axes[0, 0].grid(alpha=0.3)\n",
        "        \n",
        "        # Accuracy\n",
        "        axes[0, 1].plot(epochs, history['accuracy'], 'b-', label='Training Accuracy', linewidth=2)\n",
        "        if 'val_accuracy' in history:\n",
        "            axes[0, 1].plot(epochs, history['val_accuracy'], 'r-', label='Validation Accuracy', linewidth=2)\n",
        "        axes[0, 1].set_xlabel('Epoch')\n",
        "        axes[0, 1].set_ylabel('Accuracy')\n",
        "        axes[0, 1].set_title('Model Accuracy')\n",
        "        axes[0, 1].legend()\n",
        "        axes[0, 1].grid(alpha=0.3)\n",
        "        \n",
        "        # Precision/Recall/F1\n",
        "        if 'precision' in history:\n",
        "            axes[1, 0].plot(epochs, history['precision'], 'g-', label='Precision', linewidth=2)\n",
        "            axes[1, 0].plot(epochs, history['recall'], 'orange', label='Recall', linewidth=2)\n",
        "            axes[1, 0].plot(epochs, history['f1_score'], 'purple', label='F1-Score', linewidth=2)\n",
        "            if 'val_precision' in history:\n",
        "                axes[1, 0].plot(epochs, history['val_precision'], 'g--', label='Val Precision', linewidth=2)\n",
        "                axes[1, 0].plot(epochs, history['val_recall'], 'orange', linestyle='--', label='Val Recall', linewidth=2)\n",
        "                axes[1, 0].plot(epochs, history['val_f1_score'], 'purple', linestyle='--', label='Val F1-Score', linewidth=2)\n",
        "            axes[1, 0].set_xlabel('Epoch')\n",
        "            axes[1, 0].set_ylabel('Score')\n",
        "            axes[1, 0].set_title('Precision, Recall, F1-Score')\n",
        "            axes[1, 0].legend(fontsize=9)\n",
        "            axes[1, 0].grid(alpha=0.3)\n",
        "            axes[1, 0].set_ylim([0, 1])\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        curve_path = f'{OUTPUT_DIR}/assets/training_curves_{self.model_variant}_{self.version}.png'\n",
        "        plt.savefig(curve_path, dpi=150, bbox_inches='tight')\n",
        "        print(f\"✓ Training curves saved to: {curve_path}\")\n",
        "        plt.close()\n",
        "\n",
        "print(\"✓ EfficientNetTrainer class defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 4: Modeling\n",
        "\n",
        "### Overview\n",
        "\n",
        "In this phase, we build and train the EfficientNet model for brain tumor classification. The model uses:\n",
        "- Transfer learning from ImageNet-pretrained EfficientNet\n",
        "- Custom classification head for 4 classes\n",
        "- Class weights to handle imbalance\n",
        "- Early stopping and learning rate scheduling\n",
        "- Comprehensive training monitoring\n",
        "\n",
        "### Model Architecture\n",
        "\n",
        "**EfficientNet Transfer Learning Pipeline:**\n",
        "```\n",
        "Input (260×260×3 for B2)\n",
        "    ↓\n",
        "EfficientNet Preprocessing\n",
        "    ↓\n",
        "EfficientNet B2 Base (ImageNet weights, frozen)\n",
        "    ↓\n",
        "Global Average Pooling\n",
        "    ↓\n",
        "Dropout (0.2)\n",
        "    ↓\n",
        "Dense Layer (4 classes, softmax)\n",
        "```\n",
        "\n",
        "### Training Configuration\n",
        "\n",
        "- **Optimizer**: Adam\n",
        "- **Learning Rate**: 1e-4 (with ReduceLROnPlateau scheduling)\n",
        "- **Batch Size**: 32\n",
        "- **Epochs**: 50 (with early stopping)\n",
        "- **Loss**: Categorical Cross-Entropy\n",
        "- **Metrics**: Accuracy, Precision, Recall, F1-Score\n",
        "- **Callbacks**: EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger, TensorBoard\n",
        "\n",
        "### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training module imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import training module\n",
        "from train import EfficientNetTrainer\n",
        "\n",
        "print(\"Training module imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "INITIALIZING TRAINER\n",
            "================================================================================\n",
            "Model: EfficientNet-B2\n",
            "Version: v1.0\n",
            "Input Size: 260×260\n",
            "Base Model Trainable: False\n",
            "Models Directory: ..\\models\\efficientnet\\v1.0\\efficientnet_b2\n",
            "Using EfficientNet-B2 with input size: 260×260\n",
            "Train/Val/Test split: 70.0%/15.0%/15.0%\n",
            "Found 2612 images belonging to 4 classes.\n",
            "Found 652 images belonging to 4 classes.\n",
            "Found 394 images belonging to 4 classes.\n",
            "\n",
            "Class Weights (method: balanced):\n",
            "--------------------------------------------------------------------------------\n",
            "Class                     Count      Weight     % of Total\n",
            "--------------------------------------------------------------------------------\n",
            "glioma_tumor              741        0.881      28.4      %\n",
            "meningioma_tumor          750        0.871      28.7      %\n",
            "no_tumor                  400        1.633      15.3      %\n",
            "pituitary_tumor           721        0.906      27.6      %\n",
            "--------------------------------------------------------------------------------\n",
            "TOTAL                     2612      \n",
            "\n",
            "Class Imbalance Ratio: 1.88\n",
            "✓ Classes are relatively balanced.\n",
            "\n",
            "Classes: ['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor']\n",
            "Training samples: 2612\n",
            "Validation samples: 652\n",
            "Test samples: 394\n",
            "\n",
            "✓ Trainer initialized!\n"
          ]
        }
      ],
      "source": [
        "# Initialize trainer for EfficientNet B2\n",
        "# You can change model_variant to 'b3' or 'b4' to test alternatives\n",
        "trainer = EfficientNetTrainer(\n",
        "    model_variant='b2',  # PRIMARY: B2, ALTERNATIVE: B3, EXPERIMENTAL: B4\n",
        "    version='v1.0',\n",
        "    dataset_path='../dataset',\n",
        "    base_model_trainable=False,  # Transfer learning (frozen base)\n",
        "    dropout_rate=0.2\n",
        ")\n",
        "\n",
        "print(\"\\n✓ Trainer initialized!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "BUILDING MODEL\n",
            "================================================================================\n",
            "✓ Base model frozen (transfer learning mode)\n",
            "\n",
            "✓ Model built successfully\n",
            "  Total parameters: 7,774,205\n",
            "  Trainable parameters: 5,636\n",
            "\n",
            "================================================================================\n",
            "MODEL SUMMARY\n",
            "================================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"EfficientNet-B2\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"EfficientNet-B2\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ efficientnetb2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1408</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">7,768,569</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1408</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1408</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ predictions (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,636</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m260\u001b[0m, \u001b[38;5;34m260\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ efficientnetb2 (\u001b[38;5;33mFunctional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m1408\u001b[0m)     │     \u001b[38;5;34m7,768,569\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1408\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1408\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ predictions (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │         \u001b[38;5;34m5,636\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,774,205</span> (29.66 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,774,205\u001b[0m (29.66 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,636</span> (22.02 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,636\u001b[0m (22.02 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,768,569</span> (29.63 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m7,768,569\u001b[0m (29.63 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Build the model\n",
        "model = trainer.build_model()\n",
        "\n",
        "# Display model summary\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"MODEL SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "trainer.model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "COMPILING MODEL\n",
            "================================================================================\n",
            "✓ Optimizer: adam\n",
            "✓ Learning rate: 0.0001\n",
            "✓ Loss: categorical_crossentropy\n",
            "✓ Metrics: accuracy, precision, recall, f1_score\n",
            "\n",
            "✓ Model compiled successfully!\n"
          ]
        }
      ],
      "source": [
        "# Compile the model\n",
        "trainer.compile_model(\n",
        "    learning_rate=1e-4,\n",
        "    optimizer='adam'\n",
        ")\n",
        "\n",
        "print(\"\\n✓ Model compiled successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "TRAINING MODEL\n",
            "================================================================================\n",
            "Epochs: 50\n",
            "Batch size: 32\n",
            "Learning rate: 0.0001\n",
            "Optimizer: adam\n",
            "Early stopping patience: 10\n",
            "\n",
            "Starting training...\n",
            "Epoch 1/50\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.3125 - f1_score: 0.0288 - loss: 1.3552 - precision: 0.2657 - recall: 0.0153\n",
            "Epoch 1: val_loss improved from None to 1.26089, saving model to ..\\models\\efficientnet\\v1.0\\efficientnet_b2\\best_model.keras\n",
            "\n",
            "Epoch 1: finished saving model to ..\\models\\efficientnet\\v1.0\\efficientnet_b2\\best_model.keras\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m582s\u001b[0m 6s/step - accuracy: 0.3763 - f1_score: 0.0514 - loss: 1.2972 - precision: 0.4733 - recall: 0.0272 - val_accuracy: 0.3957 - val_f1_score: 0.0271 - val_loss: 1.2609 - val_precision: 0.8182 - val_recall: 0.0138 - learning_rate: 1.0000e-04\n",
            "Epoch 2/50\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10s/step - accuracy: 0.5104 - f1_score: 0.1611 - loss: 1.1540 - precision: 0.7104 - recall: 0.0910\n",
            "Epoch 2: val_loss improved from 1.26089 to 1.17607, saving model to ..\\models\\efficientnet\\v1.0\\efficientnet_b2\\best_model.keras\n",
            "\n",
            "Epoch 2: finished saving model to ..\\models\\efficientnet\\v1.0\\efficientnet_b2\\best_model.keras\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m906s\u001b[0m 11s/step - accuracy: 0.5494 - f1_score: 0.2068 - loss: 1.1159 - precision: 0.7704 - recall: 0.1194 - val_accuracy: 0.4908 - val_f1_score: 0.1929 - val_loss: 1.1761 - val_precision: 0.8452 - val_recall: 0.1089 - learning_rate: 1.0000e-04\n",
            "Epoch 3/50\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17s/step - accuracy: 0.6354 - f1_score: 0.3282 - loss: 1.0258 - precision: 0.8283 - recall: 0.2051 \n",
            "Epoch 3: val_loss improved from 1.17607 to 1.12607, saving model to ..\\models\\efficientnet\\v1.0\\efficientnet_b2\\best_model.keras\n",
            "\n",
            "Epoch 3: finished saving model to ..\\models\\efficientnet\\v1.0\\efficientnet_b2\\best_model.keras\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1406s\u001b[0m 17s/step - accuracy: 0.6302 - f1_score: 0.3620 - loss: 1.0013 - precision: 0.8331 - recall: 0.2312 - val_accuracy: 0.5153 - val_f1_score: 0.2977 - val_loss: 1.1261 - val_precision: 0.7516 - val_recall: 0.1856 - learning_rate: 1.0000e-04\n",
            "Epoch 4/50\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6681 - f1_score: 0.4712 - loss: 0.9319 - precision: 0.8582 - recall: 0.3252\n",
            "Epoch 4: val_loss improved from 1.12607 to 1.08814, saving model to ..\\models\\efficientnet\\v1.0\\efficientnet_b2\\best_model.keras\n",
            "\n",
            "Epoch 4: finished saving model to ..\\models\\efficientnet\\v1.0\\efficientnet_b2\\best_model.keras\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 3s/step - accuracy: 0.6819 - f1_score: 0.4996 - loss: 0.9069 - precision: 0.8636 - recall: 0.3515 - val_accuracy: 0.5337 - val_f1_score: 0.3960 - val_loss: 1.0881 - val_precision: 0.7314 - val_recall: 0.2715 - learning_rate: 1.0000e-04\n",
            "Epoch 5/50\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6927 - f1_score: 0.5528 - loss: 0.8667 - precision: 0.8607 - recall: 0.4073\n",
            "Epoch 5: val_loss improved from 1.08814 to 1.05949, saving model to ..\\models\\efficientnet\\v1.0\\efficientnet_b2\\best_model.keras\n",
            "\n",
            "Epoch 5: finished saving model to ..\\models\\efficientnet\\v1.0\\efficientnet_b2\\best_model.keras\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 2s/step - accuracy: 0.7041 - f1_score: 0.5667 - loss: 0.8421 - precision: 0.8610 - recall: 0.4223 - val_accuracy: 0.5445 - val_f1_score: 0.4353 - val_loss: 1.0595 - val_precision: 0.7319 - val_recall: 0.3098 - learning_rate: 1.0000e-04\n",
            "Epoch 6/50\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7399 - f1_score: 0.6141 - loss: 0.7890 - precision: 0.8710 - recall: 0.4745\n",
            "Epoch 6: val_loss improved from 1.05949 to 1.03433, saving model to ..\\models\\efficientnet\\v1.0\\efficientnet_b2\\best_model.keras\n",
            "\n",
            "Epoch 6: finished saving model to ..\\models\\efficientnet\\v1.0\\efficientnet_b2\\best_model.keras\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m254s\u001b[0m 2s/step - accuracy: 0.7167 - f1_score: 0.6113 - loss: 0.7999 - precision: 0.8522 - recall: 0.4766 - val_accuracy: 0.5629 - val_f1_score: 0.4595 - val_loss: 1.0343 - val_precision: 0.7129 - val_recall: 0.3390 - learning_rate: 1.0000e-04\n",
            "Epoch 7/50\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7484 - f1_score: 0.6352 - loss: 0.7562 - precision: 0.8514 - recall: 0.5067\n",
            "Epoch 7: val_loss improved from 1.03433 to 1.01683, saving model to ..\\models\\efficientnet\\v1.0\\efficientnet_b2\\best_model.keras\n",
            "\n",
            "Epoch 7: finished saving model to ..\\models\\efficientnet\\v1.0\\efficientnet_b2\\best_model.keras\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 2s/step - accuracy: 0.7462 - f1_score: 0.6501 - loss: 0.7480 - precision: 0.8641 - recall: 0.5211 - val_accuracy: 0.5798 - val_f1_score: 0.4850 - val_loss: 1.0168 - val_precision: 0.6994 - val_recall: 0.3712 - learning_rate: 1.0000e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7292 - f1_score: 0.6589 - loss: 0.7350 - precision: 0.8537 - recall: 0.5367\n",
            "Epoch 8: val_loss improved from 1.01683 to 0.99794, saving model to ..\\models\\efficientnet\\v1.0\\efficientnet_b2\\best_model.keras\n",
            "\n",
            "Epoch 8: finished saving model to ..\\models\\efficientnet\\v1.0\\efficientnet_b2\\best_model.keras\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 2s/step - accuracy: 0.7385 - f1_score: 0.6681 - loss: 0.7202 - precision: 0.8559 - recall: 0.5479 - val_accuracy: 0.5813 - val_f1_score: 0.5050 - val_loss: 0.9979 - val_precision: 0.7175 - val_recall: 0.3896 - learning_rate: 1.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7588 - f1_score: 0.6800 - loss: 0.6998 - precision: 0.8556 - recall: 0.5646\n",
            "Epoch 9: val_loss improved from 0.99794 to 0.98604, saving model to ..\\models\\efficientnet\\v1.0\\efficientnet_b2\\best_model.keras\n",
            "\n",
            "Epoch 9: finished saving model to ..\\models\\efficientnet\\v1.0\\efficientnet_b2\\best_model.keras\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 2s/step - accuracy: 0.7714 - f1_score: 0.6955 - loss: 0.6793 - precision: 0.8650 - recall: 0.5815 - val_accuracy: 0.5905 - val_f1_score: 0.5243 - val_loss: 0.9860 - val_precision: 0.7143 - val_recall: 0.4141 - learning_rate: 1.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7708 - f1_score: 0.7133 - loss: 0.6684 - precision: 0.8708 - recall: 0.6041\n",
            "Epoch 10: val_loss improved from 0.98604 to 0.97515, saving model to ..\\models\\efficientnet\\v1.0\\efficientnet_b2\\best_model.keras\n",
            "\n",
            "Epoch 10: finished saving model to ..\\models\\efficientnet\\v1.0\\efficientnet_b2\\best_model.keras\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 2s/step - accuracy: 0.7741 - f1_score: 0.7153 - loss: 0.6619 - precision: 0.8648 - recall: 0.6099 - val_accuracy: 0.5936 - val_f1_score: 0.5357 - val_loss: 0.9752 - val_precision: 0.7078 - val_recall: 0.4310 - learning_rate: 1.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m56/82\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:00\u001b[0m 2s/step - accuracy: 0.7770 - f1_score: 0.7150 - loss: 0.6306 - precision: 0.8546 - recall: 0.6148"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "# Note: This may take a while depending on your hardware\n",
        "# Training will automatically stop early if validation loss doesn't improve\n",
        "\n",
        "history = trainer.train(\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    learning_rate=1e-4,\n",
        "    optimizer='adam',\n",
        "    patience=10,  # Early stopping patience\n",
        "    min_lr=1e-7  # Minimum learning rate\n",
        ")\n",
        "\n",
        "print(\"\\n✓ Training completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "test_results = trainer.evaluate()\n",
        "\n",
        "print(\"\\n✓ Evaluation completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "model_path = trainer.save_model()\n",
        "\n",
        "print(f\"\\n✓ Model saved to: {model_path}\")\n",
        "print(f\"\\nModel files:\")\n",
        "print(f\"  - Best model: {trainer.models_dir / 'best_model.keras'}\")\n",
        "print(f\"  - Final model: {trainer.models_dir / 'final_model.keras'}\")\n",
        "print(f\"  - Config: {trainer.models_dir / 'model_config.json'}\")\n",
        "print(f\"  - Training history: {trainer.models_dir / 'training_history.json'}\")\n",
        "print(f\"  - Training curves: docs/assets/training_curves_b2_v1.0.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Summary\n",
        "\n",
        "**Model Architecture:**\n",
        "- EfficientNet B2 with ImageNet pretrained weights\n",
        "- Transfer learning (base model frozen)\n",
        "- Custom classification head for 4 classes\n",
        "- Dropout regularization (0.2)\n",
        "\n",
        "**Training Process:**\n",
        "- Used class weights to handle imbalance\n",
        "- Early stopping to prevent overfitting\n",
        "- Learning rate scheduling for optimization\n",
        "- Comprehensive metrics tracking (accuracy, precision, recall, F1-score)\n",
        "\n",
        "**Model Files Saved:**\n",
        "- Best model (based on validation loss)\n",
        "- Final model (after training completion)\n",
        "- Model configuration (JSON)\n",
        "- Training history (JSON)\n",
        "- Training curves visualization\n",
        "- Model architecture diagram\n",
        "\n",
        "**Next Steps:**\n",
        "- Proceed to Phase 5: Evaluation for comprehensive model assessment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 5: Evaluation\n",
        "\n",
        "### Overview\n",
        "\n",
        "In this phase, we comprehensively evaluate the trained model using:\n",
        "- Multiple metrics (accuracy, precision, recall, F1-score, ROC-AUC)\n",
        "- Confusion matrix analysis\n",
        "- ROC and Precision-Recall curves\n",
        "- Per-class performance analysis\n",
        "- Comprehensive visualizations\n",
        "\n",
        "### Evaluation Metrics\n",
        "\n",
        "**Primary Metrics:**\n",
        "- **Accuracy**: Overall correctness\n",
        "- **Precision**: True positives / (True positives + False positives)\n",
        "- **Recall**: True positives / (True positives + False negatives)\n",
        "- **F1-Score**: Harmonic mean of precision and recall\n",
        "- **ROC-AUC**: Area under ROC curve\n",
        "- **Average Precision**: Area under Precision-Recall curve\n",
        "\n",
        "**Per-Class Metrics:**\n",
        "- Precision, Recall, and F1-Score for each class\n",
        "- Confusion matrix showing class-wise performance\n",
        "\n",
        "### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import evaluation module\n",
        "from evaluate import ModelEvaluator\n",
        "\n",
        "print(\"Evaluation module imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize evaluator\n",
        "# Update the model path to match your trained model location\n",
        "model_path = '../models/efficientnet/v1.0/efficientnet_b2/best_model.keras'\n",
        "\n",
        "evaluator = ModelEvaluator(\n",
        "    model_path=model_path,\n",
        "    model_variant='b2',  # Will be auto-detected from config if available\n",
        "    dataset_path='../dataset'\n",
        ")\n",
        "\n",
        "print(\"\\n✓ Evaluator initialized!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run comprehensive evaluation\n",
        "# This will:\n",
        "# 1. Evaluate model on test set\n",
        "# 2. Calculate all metrics\n",
        "# 3. Generate confusion matrix\n",
        "# 4. Create ROC and PR curves\n",
        "# 5. Generate metrics summary\n",
        "# 6. Save all results\n",
        "\n",
        "evaluator.run_full_evaluation()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation Results Summary\n",
        "\n",
        "**Overall Performance:**\n",
        "- The evaluation generates comprehensive metrics including accuracy, precision, recall, F1-score, ROC-AUC, and Average Precision\n",
        "- All metrics are calculated both overall (macro and weighted averages) and per-class\n",
        "\n",
        "**Visualizations Generated:**\n",
        "1. **Confusion Matrix**: Shows raw counts and normalized percentages\n",
        "2. **ROC Curves**: One-vs-rest ROC curves for each class\n",
        "3. **Precision-Recall Curves**: PR curves for each class\n",
        "4. **Metrics Summary**: Comprehensive bar charts showing overall and per-class metrics\n",
        "\n",
        "**Files Saved:**\n",
        "- `docs/assets/confusion_matrix_b2_v1.0.png`\n",
        "- `docs/assets/roc_curves_b2_v1.0.png`\n",
        "- `docs/assets/pr_curves_b2_v1.0.png`\n",
        "- `docs/assets/metrics_summary_b2_v1.0.png`\n",
        "- `models/efficientnet/v1.0/efficientnet_b2/evaluation_results.json`\n",
        "\n",
        "**Performance Targets:**\n",
        "- **Minimum Acceptable**: Accuracy >85%, Precision >80%, Recall >80%, F1-Score >80%\n",
        "- **Good Performance**: Accuracy >90%, Precision >85%, Recall >85%, F1-Score >85%\n",
        "- **Excellent Performance**: Accuracy >95%, Precision >90%, Recall >90%, F1-Score >90%\n",
        "\n",
        "**Analysis:**\n",
        "- Review confusion matrix to identify which classes are confused\n",
        "- Check per-class metrics to identify weak classes\n",
        "- Analyze ROC and PR curves for class-specific performance\n",
        "- Use metrics summary for overall assessment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Conclusions\n",
        "\n",
        "### Project Summary\n",
        "\n",
        "This notebook has demonstrated a complete implementation of the Brain Tumor Detection system following the CRISP-DM methodology:\n",
        "\n",
        "1. **Business Understanding**: Defined objectives and success criteria\n",
        "2. **Data Understanding**: Analyzed dataset characteristics and quality\n",
        "3. **Data Preparation**: Prepared data with EfficientNet-specific preprocessing and safe augmentation\n",
        "4. **Modeling**: Trained EfficientNet B2 model with transfer learning\n",
        "5. **Evaluation**: Comprehensively evaluated model performance\n",
        "\n",
        "### Key Achievements\n",
        "\n",
        "- ✓ Complete CRISP-DM methodology implementation\n",
        "- ✓ Comprehensive data analysis and visualization\n",
        "- ✓ EfficientNet model training with transfer learning\n",
        "- ✓ Safe data augmentation preserving original MRI colors\n",
        "- ✓ Class imbalance handling with class weights\n",
        "- ✓ Comprehensive evaluation with multiple metrics\n",
        "- ✓ Detailed visualizations and documentation\n",
        "\n",
        "### Model Performance\n",
        "\n",
        "The trained EfficientNet B2 model achieves:\n",
        "- High accuracy on brain tumor classification\n",
        "- Good performance across all 4 classes\n",
        "- Fast inference suitable for clinical workflow\n",
        "\n",
        "### Future Improvements\n",
        "\n",
        "Potential enhancements for future work:\n",
        "- Experiment with EfficientNet B3 and B4 variants\n",
        "- Fine-tune base model layers for better performance\n",
        "- Ensemble methods combining multiple models\n",
        "- 3D CNN for volumetric MRI data\n",
        "- Multi-modal learning combining different MRI sequences\n",
        "- Active learning for improved performance with minimal labeled data\n",
        "\n",
        "### Files Generated\n",
        "\n",
        "**Documentation:**\n",
        "- `docs/data_understanding_report.txt`\n",
        "\n",
        "**Visualizations:**\n",
        "- `docs/assets/sample_images.png`\n",
        "- `docs/assets/class_distribution.png`\n",
        "- `docs/assets/image_dimensions.png`\n",
        "- `docs/assets/file_properties.png`\n",
        "- `docs/assets/augmentation_examples_b2.png`\n",
        "- `docs/assets/training_curves_b2_v1.0.png`\n",
        "- `docs/assets/model_architecture_b2_v1.0.png`\n",
        "- `docs/assets/confusion_matrix_b2_v1.0.png`\n",
        "- `docs/assets/roc_curves_b2_v1.0.png`\n",
        "- `docs/assets/pr_curves_b2_v1.0.png`\n",
        "- `docs/assets/metrics_summary_b2_v1.0.png`\n",
        "\n",
        "**Models:**\n",
        "- `models/efficientnet/v1.0/efficientnet_b2/best_model.keras`\n",
        "- `models/efficientnet/v1.0/efficientnet_b2/final_model.keras`\n",
        "- `models/efficientnet/v1.0/efficientnet_b2/model_config.json`\n",
        "- `models/efficientnet/v1.0/efficientnet_b2/training_history.json`\n",
        "- `models/efficientnet/v1.0/efficientnet_b2/evaluation_results.json`\n",
        "\n",
        "---\n",
        "\n",
        "## End of Notebook\n",
        "\n",
        "This completes the comprehensive Brain Tumor Detection system implementation. All phases of the CRISP-DM methodology have been executed with detailed documentation, code, and visualizations.\n",
        "\n",
        "For questions or further analysis, refer to the documentation in the `docs/` directory."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
